Thinking1：在推荐系统中，FM和MF哪个应用的更多，为什么
答：FM应用更多。FM考虑了包含user和item在内的多个特征及两两特征间的交互作用，而MF只考虑user和item特征，MF可看作是FM的特例。
    但是，FM在计算两两特征交互作用的系数时使用了MF。

Thinking2：FFM与FM有哪些区别？
答：FM将每个特征一个隐向量，而FFM将一个特征（即"场"）中的不同属性值分开考虑，由此得到多个隐向量，FM也可以视为FFM的特例。

Thinking3：DeepFM相比于FM解决了哪些问题，原理是怎样的
答：FM可以做特征组合，但考虑到计算量一般只进行2阶的特征组合。DeepFM则既考虑了低阶（1阶+2阶）特征，又能考虑到高阶特征组合。
    DeepFM的原理组合了FM和DNN，两者共享特征输入，但中间过程分别单独进行，其中FM就用来进行低阶特征的建模计算，
    而DNN则是需要设计特定的子网络结构（从输入层=>嵌入层），将原始的稀疏表示特征映射为稠密的特征向量（sparse features => dense Embedding），以更好的发挥DNN模型学习高阶特征的能力，
    最后将二者的结果结合，再进行sigmoid转换，得到预测结果。

Thinking4：Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？
答：Surpise中baseline算法原理是设立基线（系统整体均值miu），并引入用户的系统偏差bu以及商品系统偏好bi，来实现user对某一具体item的评分预测，即bui = miu + bu + bi；
    BaselineOnly就是baseline算法的实现，而KNNBaseline则是首先找到用户或商品的k个近邻，在所有近邻中使用baseline算法，这里的miu、bu和bi不再是从整个系统中计算得到的，而是从近邻中得到的。

Thinking5：基于邻域的协同过滤都有哪些算法，请简述原理
答：基于邻域的协同过滤包含UserCF和ItemCF算法。
    UserCF：寻找和当前用户相似度高的N个用户，从这N个用户感兴趣的物品中，找到预测评分最高的物品给当前用户，其本质是利用用户行为的相似度计算用户的相似度，推荐相似用户感兴趣的物品
            Step1 找近邻，即找到和目标用户兴趣行为相似的k个邻居集合，相似度计算方法有Jaccard相似度计算、余弦相似度、增加流行度惩罚项的改进相似度；
            Step2 计算用户对潜在物品的感兴趣程度，基于k个邻居对某物品i的兴趣，计算目标用户对该物品i的兴趣，即 目标用户u与用户v的相似度 * 用户v对物品i的“行为”
            Step3 生成推荐列表，把和用户行为相同的k个邻居喜欢的物品进行汇总，去掉用户u已经喜欢过的物品，剩下按照从大到小进行推荐。
    ItemCF：基于目标用户历史上感兴趣过的物品，寻找与之相似的新物品作为推荐，但这里物品的相似度是通过用户行为判断的，而不是物品的物理特征上的相似度
            Step1 找近邻，即寻找与用户感兴趣过的某已知物品相似的k个邻居
            Step2 计算用户对潜在物品的感兴趣程度，即 目标用户u对已知物品i的兴趣度 * 物品i与物品j的相似度
            Step3 为用户u生成推荐列表，去掉用户u已经喜欢过的物品，剩下按照从大到小进行推荐
