Thinking1：在CTR点击率预估中，使用GBDT+LR的原理是什么？
答：最基础的CTR预估采用LR模型，LR是线性模型，但是利用特征有限，导致模型学习能力有限；预提高模型预测能力，只有通过人工特征区分、组合等实现，但对人员能力要求较高，且带来较高时间、人工成本。
    GBDT+LR，则是采取了集成学习中的stacking思想，首先使用GBDT自动发现有效的特征及特征组合，弥补人工经验不足，再将GBDT的预测结果作为特征输入到LR模型中，以此提高CTR预估准确性。
    这里，当GBDT的结果输出并不是平时常见的二分类概率值，而是把每个样本的通过每棵树计算得到的预测概率值所属的叶子结点位置记为1，以此形成类似于one-hot编码的新特征数据，这样得到的特征数据能够
    较好的反映特征组合，同时也能够区分相对的重要性。

Thinking2：Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）
答：wide & deep模型分为wide learning和deep learning两个子模型，其中wide learning就是lr模型，而deep learning就是DNN。
   模型原理：首先分别将特征输入到这两个模型中，再将得到的两个结果进行加和，然后再做sigmoid变换，得到预测的概率值。
   模型的memorization主要来自LR，记录原有特征和结果的相关性，实现“记忆”；模型的generalization主要来自DNN，通过历史的相关性信息，去探索一些在过去没有的特征组合，进行“推理”。

Thinking3：在CTR预估中，使用FM与DNN有哪些结合的方式，代表模型有哪些？
答：FM和DNN的结合方式包含并行和串行两种形式：
   并行结构的代表模型为DeepFM，是特征分别输入FM和DNN，分开计算得到结果后再进行合并处理；
   串行结构的代表模型为NFM，是首先将特征进行FM，然后以二阶组合的结果作为DNN的输入，进行高阶特征组合的预测计算。

Thinking4：GBDT和随机森林都是基于树的算法，它们有什么区别？
答：RF和GBDT虽然都是ensemble的树模型，但RF是基于bagging思想，GBDT是基于boosting思想：
    1）bagging是通过自助采样（boostrap）方式得到多组样本，每一组样本建立一棵树，最后将所有的树的结果进行汇总，采取“少数服从多数”（voting）的方式确定最终的结果，
    由于RF中每一个树是基于各自的样本组，因此每棵树之间实际是相互独立的，因此可以并行计算；
    2）boosting虽然也是将多棵树合成一棵树的算法，但与RF有两点不同：首先boosting使用的所有样本，没有抽样的过程；其次，boosting总体上看是一个线性的加法模型，即是将一棵棵树累加起来
    而每棵通过特定算法分配一定的权重（预测越准确权重越高），同时树之间有着严格的顺序关系，因为后一棵树的预测目标来自于前一棵树的预测结果，如adaboost中后一棵树的关注前一棵树中预测错的样本，
    GBDT中后一棵树则是基于前一棵树残差的梯度。

Thinking5：item流行度在推荐系统中有怎样的应用
答：item流行度，本质上就是俗称的“热门”。在冷启动场景中，由于用户数据较为稀疏，多采取基于流行度的推荐，即推荐热门；
    但随着用户数据的积累，用户会更加需要长尾推荐，需要考虑高流行度对商品个性化推荐的降权影响。

